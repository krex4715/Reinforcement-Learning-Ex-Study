{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 예제 3.5번(임의 탐색)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/문제상황.png\" style=\"width: 500px;\" align=\"left\"/>\n",
    "\n",
    "\n",
    "왼쪽과 같은 문제상황.    \n",
    "5X5 Grid안에서    \n",
    "Action은 상,하,좌,우가 가능함 \n",
    "       \n",
    "이때 5X5 \"State Value Table\"을 오른쪽 그림과 같이 만들어 내는것이 문제   \n",
    "\n",
    "-> Continous Task가 맞으나, 10000번중 한번은 위치 초기화 해줬음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Value Table vs State Value Table\n",
    "- Action Value 방식은 Action Value에 대한Table과, Reward 파악을 위한 각 State별 Reward Table 두가지를 별도로 만들어줘야함.     \n",
    "- 반면 State Value는 하나만 있으면 됨-> State Table에 그대로 Reward를 기입해주면 되기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 풀이시작"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 환경에서 정의되어야 하는 것\n",
    "- Reward에 대한 정의\n",
    "- 행동, 상태에대한 정의 -> 에이전트의 각 행동선택에대한 State의 변화 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gridworld:\n",
    "    def __init__(self, width = 5 , height = 5):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.grid = np.zeros((self.width,self.height))\n",
    "        self.current_location = (4,np.random.randint(0,5))\n",
    "        # self.current_location = (2,2)\n",
    "        \n",
    "        self.grid[0,1] = 10\n",
    "        self.grid[0,3] = 5\n",
    "        \n",
    "        self.actions = np.array(['UP', 'DOWN', 'LEFT', 'RIGHT'])\n",
    "        \n",
    "    # For Debugging\n",
    "    def agent_on_map(self):\n",
    "        grid = np.zeros(( self.height, self.width))\n",
    "        grid[ self.current_location[0], self.current_location[1]] = 1\n",
    "        \n",
    "        grid[0,1] = 10\n",
    "        grid[0,3] = 5\n",
    "        return grid\n",
    "        \n",
    "        \n",
    "    def get_reward(self , location):\n",
    "        # print('location : ',location)\n",
    "        # print('reward : ',self.grid[location[0],location[1]])\n",
    "        return self.grid[location[0],location[1]]\n",
    "    \n",
    "    def make_step(self,action):\n",
    "        reward=0\n",
    "        before_location = self.current_location\n",
    "        x = before_location[0]\n",
    "        y = before_location[1]\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        if action == 'UP':\n",
    "            if self.current_location[0]>0:\n",
    "                self.current_location = (self.current_location[0]-1,self.current_location[1])\n",
    "                reward = self.get_reward(self.current_location)\n",
    "                if self.current_location == (0,1):\n",
    "                    self.current_location = (4,1)\n",
    "                elif self.current_location == (0,3):\n",
    "                    self.current_location = (3,3)\n",
    "\n",
    "            \n",
    "        elif action == 'DOWN':\n",
    "            if self.current_location[0]<4:\n",
    "                self.current_location = (self.current_location[0]+1,self.current_location[1])\n",
    "                reward = self.get_reward(self.current_location)\n",
    "                if self.current_location == (0,1):\n",
    "                    self.current_location = (4,1)\n",
    "                elif self.current_location == (0,3):\n",
    "                    self.current_location = (3,3)\n",
    "\n",
    "            \n",
    "        elif action == 'LEFT':\n",
    "            if self.current_location[1]>0:\n",
    "                self.current_location = (self.current_location[0],self.current_location[1]-1)\n",
    "                reward = self.get_reward(self.current_location)\n",
    "                if self.current_location == (0,1):\n",
    "                    self.current_location = (4,1)\n",
    "                elif self.current_location == (0,3):\n",
    "                    self.current_location = (3,3)\n",
    "\n",
    "            \n",
    "        elif action == 'RIGHT':\n",
    "            if self.current_location[1]<4:\n",
    "                self.current_location = (self.current_location[0],self.current_location[1]+1)\n",
    "                reward = self.get_reward(self.current_location)\n",
    "                if self.current_location == (0,1):\n",
    "                    self.current_location = (4,1)\n",
    "                elif self.current_location == (0,3):\n",
    "                    self.current_location = (3,3)\n",
    "\n",
    "        # print(action)\n",
    "        return reward,action\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 에이전트에서 정의되어야 하는것\n",
    "- 행동을 선택하는 방법에대한 정의\n",
    "- 학습 과정에 대한 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateValue_Agent():\n",
    "    def __init__(self, environment, epsilon=0.05,gamma=0.9):\n",
    "        self.environment = environment\n",
    "        self.value_table = np.zeros( (environment.width , environment.height) )\n",
    "        self.value_table = environment.grid\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    \n",
    "    def action_choosing(self,available_actions):\n",
    "        reward = 0\n",
    "        if np.random.uniform(0,1) < self.epsilon:\n",
    "            action = available_actions[np.random.randint(0,len(available_actions))] # 4개중 하나 랜덤하게 택 1 \n",
    "        else:\n",
    "            action_list = []\n",
    "            reward_list = []\n",
    "            for select_action in available_actions:\n",
    "                # print(select_action)\n",
    "                reward, action = self.for_one_step_search(select_action)\n",
    "                action_list.append(action)\n",
    "                reward_list.append(reward)\n",
    "                reward = max(reward_list)\n",
    "                action = action_list[np.argmax(reward_list)] # one Step Ahead Search중 Max값\n",
    "            \n",
    "        return action,reward\n",
    "    \n",
    "    def for_one_step_search(self,action):\n",
    "        reward=0\n",
    "        location_search = environment.current_location\n",
    "        x = location_search[0]\n",
    "        y = location_search[1]\n",
    "        \n",
    "        \n",
    "        if action == 'UP':\n",
    "            if location_search[0]>0:\n",
    "                location_search = (location_search[0]-1,location_search[1])\n",
    "                reward = self.value_table[location_search]\n",
    "            \n",
    "        elif action == 'DOWN':\n",
    "            if location_search[0]<4:\n",
    "                location_search = (location_search[0]+1,location_search[1])\n",
    "                reward = self.value_table[location_search]\n",
    "            \n",
    "        elif action == 'LEFT':\n",
    "            if location_search[1]>0:\n",
    "                location_search = (location_search[0],location_search[1]-1)\n",
    "                reward = self.value_table[location_search]\n",
    "\n",
    "        elif action == 'RIGHT':\n",
    "            if location_search[1]<4:\n",
    "                location_search = (location_search[0],location_search[1]+1)\n",
    "                reward = self.value_table[location_search]\n",
    "        \n",
    "        return reward,action\n",
    "    \n",
    "    \n",
    "    \n",
    "    def learning(self,old_state,new_state,reward):\n",
    "        new_state_value = self.value_table[new_state[0],new_state[1]]\n",
    "        \n",
    "        self.value_table[old_state[0],old_state[1]] = reward + self.gamma*new_state_value\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이걸 실행시킬때 유의사항들\n",
    "- 환경과 Agent 사이 상호작용을 적절히 분배해주어야함\n",
    "- Action에 대한 선택은 Agent에 정의되어있고, 그에대한 결과는 환경에정의되어있음\n",
    "- 그리고 Value Table자체는 Agent에 정의되어있음\n",
    "- Episodic Task라면, Episode를 정의, Continous Task라면 얼만큼의 Step을 실행시킬 건지 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(environment , agent, trials = 10000, learn = False):\n",
    "    reward_list = []\n",
    "    reward = 0\n",
    "    for trial in range(trials):\n",
    "        \n",
    "        # 한번의 Try시에 거치는 단계들\n",
    "        old_state = environment.current_location\n",
    "        action,_ = agent.action_choosing(environment.actions)\n",
    "        reward,_ = environment.make_step(action)\n",
    "        new_state = environment.current_location\n",
    "        \n",
    "        grid_state_agent = environment.agent_on_map()\n",
    "        \n",
    "        # print('old_state : ',old_state)\n",
    "        # print('Choosed Action : ',action)\n",
    "        # print('그에대한 reward : ',reward)\n",
    "        # print('new_state s` : ',new_state)\n",
    "        # print(grid_state_agent)\n",
    "        # print('----------')\n",
    "        \n",
    "        if learn == True:\n",
    "            agent.learning(old_state,new_state,reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = gridworld()\n",
    "agent = StateValue_Agent(environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10000번 움직이면 초기 위치 재정의 하고 다시시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    environment.current_location = (4,np.random.randint(0,5))\n",
    "    play(environment,agent,trials=10000,learn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15.4, 10. , 14.7,  5. ,  8.2],\n",
       "       [13.8,  1. ,  8.6,  0. ,  4.9],\n",
       "       [ 8.7,  7.5,  8.2,  1.7,  0.5],\n",
       "       [ 7.3,  1.4,  1.5,  1.6,  1.4],\n",
       "       [ 6. ,  5.8,  1.5,  1.7,  1.2]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(agent.value_table,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jh",
   "language": "python",
   "name": "jh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25d2e261a005f7b328bcadb95c12a7faf97193533edd484517e57de31019042d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
